{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "import time\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f455a041af0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For both cpu and gpu integration all the variables and models should use\n",
    "# \"xx.to(device)\"  \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Basic parameters for reproducablity\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if device == \"cuda:0\": \n",
    "    torch.cuda.manual_seed_all(1) # gpu vars\n",
    "\n",
    "    # Dataset paths\n",
    "celea_dataset = \"Dataset/HIGH/celea_60000_SFD\"\n",
    "sr_dataset = \"Dataset/HIGH/SRtrainset_2\"\n",
    "vgg_dataset = \"Dataset/HIGH/vggface2/vggcrop_train\"\n",
    "\n",
    "# Model saving paths\n",
    "hightolow_generator = \"Checkpoint/hightolow_g_\"\n",
    "hightolow_discriminator = \"Checkpoint/hightolow_d_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the parameters and dynamic numbers will be setted here\n",
    "hightolow_batch_size = 8 \n",
    "epoch = 200\n",
    "learning_rate = 1e-4\n",
    "loss_a_coeff = 1\n",
    "loss_b_coeff = 0.05\n",
    "adam_beta1 = 0\n",
    "adam_beta2 = 0.9\n",
    "# After each this value of iterations generator will be updated\n",
    "# In original paper it is 5:1\n",
    "generator_update_ratio = 5 \n",
    "# Save the generated images after some interval\n",
    "# to visualize the progress\n",
    "sample_images = 400\n",
    "\n",
    "high_image_size = 64\n",
    "low_image_size = 16\n",
    "noise_dimension = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model_path,epoch,model,optimizer,loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            },  model_path + epoch)\n",
    "\n",
    "# TODO: Instead of using function to load paste these codes to main loop    \n",
    "def load_checkpoint(model_path,epoch):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    model.eval()\n",
    "    # - or -\n",
    "    model.train()\n",
    "    \n",
    "def batch_to_image(batch):\n",
    "    np_grid = vutils.make_grid(batch).numpy()\n",
    "    plt.imshow(np.transpose(np_grid, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "# Noise distrubition sampled from normal distribution\n",
    "def create_noise():\n",
    "    return torch.randn(hightolow_batch_size,64)\n",
    "\n",
    "def calculate_remaining_training_time(batches_done,batches_left):\n",
    "    print()\n",
    "    #batches_done = epoch * len(dataloader) + i\n",
    "    #batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
    "    #time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for high images\n",
    "def load_image(path):\n",
    "    return Image.open(path)\n",
    "\n",
    "# This dataset use the image datasets where images\n",
    "# are located on the dataset folder and this\n",
    "# Generic dataset should be customized according to the dataset\n",
    "# csv based dataset require different loading function\n",
    "\n",
    "class HighDataset(Dataset):\n",
    "    \"\"\" Initialize the dataset by giving the dataset path and transform that will be applied \"\"\"\n",
    "    def __init__(self,transform = None):\n",
    "        images = []\n",
    "        celea_subjects = [subject for subject in os.listdir(celea_dataset)]\n",
    "        sr_subjects = [subject for subject in os.listdir(sr_dataset)]\n",
    "        vgg_subjects = [subject for subject in os.listdir(vgg_dataset)]\n",
    "        \n",
    "        for subject in celea_subjects:\n",
    "            images.append(os.path.join(celea_dataset,subject))\n",
    "\n",
    "        for subject in sr_subjects:\n",
    "            images.append(os.path.join(sr_dataset,subject))\n",
    "                              \n",
    "        for subject in celea_subjects:\n",
    "            images.append(os.path.join(vgg_dataset,subject))\n",
    "        \n",
    "                              \n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "        self.count = len(images)\n",
    "        \n",
    "\n",
    "    \"\"\" Image with given index will be loaded by using the image path \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = load_image(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=False, upsample=False, nobn = False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.downsample = downsample\n",
    "        self.nobn = nobn\n",
    "        if self.upsample:\n",
    "            self.conv1 = nn.ConvTranspose2d(inplanes, planes, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        if not self.nobn:\n",
    "            self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if self.downsample:\n",
    "            self.conv2 =nn.Sequential(nn.AvgPool2d(2,2), conv3x3(planes, planes))\n",
    "        else:\n",
    "            self.conv2 = conv3x3(planes, planes)\n",
    "        if not self.nobn:\n",
    "            self.bn2 = nn.BatchNorm2d(planes)\n",
    "        if inplanes != planes or self.upsample or self.downsample:\n",
    "            if self.upsample:\n",
    "                self.skip = nn.ConvTranspose2d(inplanes, planes, 4, 2, 1)\n",
    "            elif self.downsample:\n",
    "                self.skip = nn.Sequential(nn.AvgPool2d(2,2), nn.Conv2d(inplanes, planes, 1, 1))\n",
    "            else:\n",
    "                self.skip = nn.Conv2d(inplanes, planes, 1, 1, 0)\n",
    "        else:\n",
    "            self.skip = None\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if not self.nobn:\n",
    "            out = self.bn1(x)\n",
    "            out = self.relu(out)\n",
    "        else:\n",
    "            out = self.relu(x)\n",
    "        out = self.conv1(out)\n",
    "        if not self.nobn:\n",
    "            out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.skip is not None:\n",
    "            residual = self.skip(x)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "    # Upsamplings can be changed with pixelShuffle\n",
    "class HighToLowGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HighToLowGenerator, self).__init__()\n",
    "        self.layers_in = conv3x3(3, 64)\n",
    "        # 64x64\n",
    "        self.residual1 = nn.Sequential(BasicBlock(64,64),BasicBlock(64,64,downsample=True))\n",
    "        # 32x32\n",
    "        self.residual2 = nn.Sequential(BasicBlock(64,64),BasicBlock(64,64,downsample=True))\n",
    "        # 16x16\n",
    "        self.residual3 = nn.Sequential(BasicBlock(64,64),BasicBlock(64,64,downsample=True))\n",
    "        # 8x8\n",
    "        self.residual4 = nn.Sequential(BasicBlock(64,64),BasicBlock(64,64,downsample=True))\n",
    "        # 4x4\n",
    "        self.residual5 = nn.Sequential(BasicBlock(64,64),BasicBlock(64,64,upsample=True))\n",
    "        # 8x8\n",
    "        self.residual6 = nn.Sequential(BasicBlock(64,64),BasicBlock(64,3,upsample=True),nn.Tanh())\n",
    "        # 16x16\n",
    "        \n",
    "        \n",
    "    def forward(self, input, noise= None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn([input.size()[0],3,64,64])\n",
    "        \n",
    "        x = input + noise \n",
    "        x = self.layers_in(x)\n",
    "        x = self.residual1(x)\n",
    "        x = self.residual2(x)\n",
    "        x = self.residual3(x)\n",
    "        x = self.residual4(x)\n",
    "        x = self.residual5(x)\n",
    "        out = self.residual6(x)\n",
    "        return out\n",
    "        \n",
    "class LowDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LowDiscriminator, self).__init__()\n",
    "\n",
    "\n",
    "        self.disc = nn.Sequential(BasicBlock(3,6,nobn=True),\n",
    "                                  BasicBlock(6,6,nobn=True),\n",
    "                                  BasicBlock(6,12,nobn=True),\n",
    "                                  BasicBlock(12,12,nobn=True),\n",
    "                                  BasicBlock(12,6,nobn=True),\n",
    "                                  BasicBlock(6,6,nobn=True))  \n",
    "        self.linear = nn.Sequential(nn.Linear(16*16*6, 1),\n",
    "                                    nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,input):\n",
    "        x = self.disc(input)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = HighDataset(transform)\n",
    "data_loader = DataLoader(dataset,batch_size = hightolow_batch_size)\n",
    "fixed_noise = torch.randn([hightolow_batch_size,3,64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-749236ddb9a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcur_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Train discriminator with real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-83e2a86f2ca2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-83e2a86f2ca2>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataset for high images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This dataset use the image datasets where images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2777\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "GANLoss = nn.HingeEmbeddingLoss().to(device)\n",
    "L2Loss = nn.MSELoss().to(device)\n",
    "hightolow_generator = HighToLowGenerator().to(device)\n",
    "hightolow_discriminator = LowDiscriminator().to(device)\n",
    "optimizer_G = torch.optim.Adam(hightolow_generator.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2))\n",
    "optimizer_D = torch.optim.Adam(hightolow_discriminator.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2))\n",
    "\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    it = iter(dataset)\n",
    "    nxt_batch = next(it)\n",
    "    samples = hightolow_generator(nxt_batch, fixed_noise)\n",
    "    vutils.save_image(samples, \"Checkpoint/%s.png\" % batches_done ,n_rows=3)\n",
    "\n",
    "batches_done = 0  \n",
    "valid = Tensor(np.ones((hightolow_batch_size, 1))), requires_grad=False)\n",
    "fake =  Tensor(np.zeros((hightolow_batch_size, 1))), requires_grad=False)\n",
    "for cur_epoch in range(epoch):\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        \n",
    "        # Train discriminator with real\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = hightolow_discriminator(batch)\n",
    "        loss_D = L2Loss(pred_real,valid)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train discriminator with fake\n",
    "        hightolow_generator.eval()\n",
    "        fake_batch = hightolow_generator(batch)\n",
    "        pred_fake = hightolow_discriminator(fake_batch)\n",
    "        loss_D = L2Loss(pred_fake,fake)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        batches_done += 1\n",
    "        # Train generator if ratio is reached\n",
    "        if i % generator_update_ratio == 0:\n",
    "            # Train it\n",
    "            hightolow_generator.train()\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_batch = hightolow_generator(batch)\n",
    "            loss_G = GANLoss(fake_batch,)\n",
    "            \n",
    "            \n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Log the progress\n",
    "            # Calculate remaining time\n",
    "            info = \"===> Epoch[{}]({}/{}): time: {:4.4f}:\".format(cur_epoch, i, len(data_loader), time.time()-start_time)\n",
    "            #info += \"Generator: {:.4f}, Discriminator: {:.4f}\".format(loss_gn, loss_dc)     \n",
    "\n",
    "            # If at sample interval sample and save images\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done)\n",
    "                \n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5255],\n",
      "        [0.5240],\n",
      "        [0.5297],\n",
      "        [0.5271],\n",
      "        [0.5275],\n",
      "        [0.5230],\n",
      "        [0.5298],\n",
      "        [0.5237]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "it = iter(data_loader)\n",
    "batch = next(it)\n",
    "model = HighToLowGenerator(random_noise=True)\n",
    "out = model(batch)\n",
    "model2 = LowDiscriminator()\n",
    "predict = model2(out)\n",
    "print(predict)\n",
    "vutils.save_image(out,\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
