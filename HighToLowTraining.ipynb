{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7e541490b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For both cpu and gpu integration all the variables and models should use\n",
    "# \"xx.to(device)\"  \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Basic parameters for reproducablity\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "if device == \"cuda:0\": \n",
    "    torch.cuda.manual_seed_all(1) # gpu vars\n",
    "# Dataset paths\n",
    "celea_dataset = \"Dataset/HIGH/celea_60000_SFD\"\n",
    "sr_dataset = \"Dataset/HIGH/SRtrainset_2\"\n",
    "vgg_dataset = \"Dataset/HIGH/vggface2/vggcrop_train\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the parameters and dynamic numbers will be setted here\n",
    "hightolow_batch_size = 8 \n",
    "epoch = 200\n",
    "learning_rate = 1e-4\n",
    "loss_a_coeff = 1\n",
    "loss_b_coeff = 0.05\n",
    "adam_beta1 = 0\n",
    "adam_beta2 = 0.9\n",
    "\n",
    "high_image_size = 64\n",
    "low_image_size = 16\n",
    "noise_dimension = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_image(batch):\n",
    "    np_grid = vutils.make_grid(batch).numpy()\n",
    "    plt.imshow(np.transpose(np_grid, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "# Noise distrubition sampled from normal distribution\n",
    "def create_noise():\n",
    "    return torch.randn(hightolow_batch_size,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for high images\n",
    "def load_image(path):\n",
    "    return Image.open(path)\n",
    "\n",
    "# This dataset use the image datasets where images\n",
    "# are located on the dataset folder and this\n",
    "# Generic dataset should be customized according to the dataset\n",
    "# csv based dataset require different loading function\n",
    "\n",
    "class HighDataset(Dataset):\n",
    "    \"\"\" Initialize the dataset by giving the dataset path and transform that will be applied \"\"\"\n",
    "    def __init__(self,transform = None):\n",
    "        images = []\n",
    "        celea_subjects = [subject for subject in os.listdir(celea_dataset)]\n",
    "        sr_subjects = [subject for subject in os.listdir(sr_dataset)]\n",
    "        vgg_subjects = [subject for subject in os.listdir(vgg_dataset)]\n",
    "        \n",
    "        for subject in celea_subjects:\n",
    "            images.append(os.path.join(celea_dataset,subject))\n",
    "\n",
    "        for subject in sr_subjects:\n",
    "            images.append(os.path.join(celea_dataset,subject))\n",
    "                              \n",
    "        for subject in celea_subjects:\n",
    "            images.append(os.path.join(celea_dataset,subject))\n",
    "        \n",
    "                              \n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "        self.count = len(images)\n",
    "        \n",
    "\n",
    "    \"\"\" Image with given index will be loaded by using the image path \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = load_image(image_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = HighDataset(transform)\n",
    "data_loader = DataLoader(dataset,batch_size = hightolow_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class HighToLowGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HighToLowGenerator, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
